{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72b1f71",
   "metadata": {},
   "source": [
    "# EDA – Seq2Seq Dataset (src/tgt)\n",
    "\n",
    "This notebook performs quick exploratory data analysis for a sequence-to-sequence dataset\n",
    "with **TSV files** and **columns**:\n",
    "- `src` – input text\n",
    "- `tgt` – target text\n",
    "\n",
    "It assumes files live under:\n",
    "```\n",
    "data/processed/train.tsv\n",
    "data/processed/dev.tsv\n",
    "data/processed/test.tsv\n",
    "```\n",
    "You can change the paths below if needed.\n",
    "\n",
    "**What you'll get:**\n",
    "- Basic dataset sizes and missing-value checks\n",
    "- Token length distributions (src & tgt)\n",
    "- Character length distributions\n",
    "- Vocabulary size (whitespace tokenization by default)\n",
    "- Sample pairs and longest/shortest examples\n",
    "- N‑gram frequency peek (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e04f97e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T00:06:26.442048Z",
     "start_time": "2025-09-26T00:06:26.425072Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Never set explicit colors per the plotting rules\n",
    "plt.rcParams.update({'figure.dpi': 120})\n",
    "\n",
    "# Paths (edit as needed)\n",
    "TRAIN_PATH = Path('data/processed/train.tsv')\n",
    "DEV_PATH   = Path('data/processed/dev.tsv')\n",
    "TEST_PATH  = Path('data/processed/test.tsv')\n",
    "\n",
    "SRC_COL = 'src'\n",
    "TGT_COL = 'tgt'\n",
    "\n",
    "def read_tsv(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {p}. Please adjust the path or add the file.\")\n",
    "    return pd.read_csv(p, sep='\\t', dtype=str).fillna('')\n",
    "\n",
    "train = read_tsv(TRAIN_PATH)\n",
    "dev   = read_tsv(DEV_PATH)\n",
    "test  = read_tsv(TEST_PATH)\n",
    "\n",
    "for name, df in [('train', train), ('dev', dev), ('test', test)]:\n",
    "    if SRC_COL not in df.columns or TGT_COL not in df.columns:\n",
    "        raise ValueError(f\"{name}.tsv must have columns '{SRC_COL}' and '{TGT_COL}' – got {list(df.columns)}\")\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing file: data/processed/train.tsv. Please adjust the path or add the file.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 23\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing file: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please adjust the path or add the file.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mread_csv(p, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m)\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 23\u001B[0m train \u001B[38;5;241m=\u001B[39m \u001B[43mread_tsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m dev   \u001B[38;5;241m=\u001B[39m read_tsv(DEV_PATH)\n\u001B[1;32m     25\u001B[0m test  \u001B[38;5;241m=\u001B[39m read_tsv(TEST_PATH)\n",
      "Cell \u001B[0;32mIn[5], line 20\u001B[0m, in \u001B[0;36mread_tsv\u001B[0;34m(p)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mread_tsv\u001B[39m(p: Path) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m p\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m---> 20\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing file: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please adjust the path or add the file.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mread_csv(p, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m)\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: Missing file: data/processed/train.tsv. Please adjust the path or add the file."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes and basic info\n",
    "def dataset_info(df: pd.DataFrame, name: str):\n",
    "    print(f\"=== {name.upper()} ===\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "    print(\"Nulls per column:\\n\", df[[SRC_COL, TGT_COL]].isnull().sum())\n",
    "    print(\"Empty strings per column:\\n\", (df[[SRC_COL, TGT_COL]] == '').sum())\n",
    "    print('-'*40)\n",
    "\n",
    "dataset_info(train, 'train')\n",
    "dataset_info(dev, 'dev')\n",
    "dataset_info(test, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cab1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token & character lengths\n",
    "def to_tokens(s: str):\n",
    "    # whitespace split – replace with your tokenizer if needed\n",
    "    return s.split()\n",
    "\n",
    "def add_lengths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out['src_char_len'] = out[SRC_COL].str.len()\n",
    "    out['tgt_char_len'] = out[TGT_COL].str.len()\n",
    "    out['src_tok_len']  = out[SRC_COL].apply(lambda x: len(to_tokens(x)))\n",
    "    out['tgt_tok_len']  = out[TGT_COL].apply(lambda x: len(to_tokens(x)))\n",
    "    return out\n",
    "\n",
    "train_len = add_lengths(train)\n",
    "dev_len   = add_lengths(dev)\n",
    "test_len  = add_lengths(test)\n",
    "\n",
    "def plot_hist(series, title, bins=50):\n",
    "    plt.figure()\n",
    "    series.hist(bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(train_len['src_tok_len'], 'Train: Source token length')\n",
    "plot_hist(train_len['tgt_tok_len'], 'Train: Target token length')\n",
    "plot_hist(train_len['src_char_len'], 'Train: Source char length')\n",
    "plot_hist(train_len['tgt_char_len'], 'Train: Target char length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09423d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "pd.set_option('display.max_colwidth', 160)\n",
    "\n",
    "print('Random samples:')\n",
    "display(train.sample(min(5, len(train)), random_state=42)[[SRC_COL, TGT_COL]])\n",
    "\n",
    "print('\\nShortest by src tokens:')\n",
    "display(train_len.nsmallest(5, 'src_tok_len')[[SRC_COL, TGT_COL, 'src_tok_len']])\n",
    "\n",
    "print('\\nLongest by src tokens:')\n",
    "display(train_len.nlargest(5, 'src_tok_len')[[SRC_COL, TGT_COL, 'src_tok_len']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953091a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple vocabulary stats (whitespace tokens)\n",
    "from collections import Counter\n",
    "\n",
    "def vocab_stats(df: pd.DataFrame, col: str, top_k=20):\n",
    "    toks = []\n",
    "    for s in df[col].tolist():\n",
    "        toks.extend(s.split())\n",
    "    counter = Counter(toks)\n",
    "    print(f\"Unique tokens in {col}: {len(counter)}\")\n",
    "    print(f\"Top {top_k} tokens:\")\n",
    "    for tok, cnt in counter.most_common(top_k):\n",
    "        print(f\"{tok}\\t{cnt}\")\n",
    "    return counter\n",
    "\n",
    "print('=== Source vocab (train) ===')\n",
    "src_counter = vocab_stats(train, SRC_COL)\n",
    "\n",
    "print('\\n=== Target vocab (train) ===')\n",
    "tgt_counter = vocab_stats(train, TGT_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf9353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: quick n-gram peek (character n-grams)\n",
    "from collections import Counter\n",
    "\n",
    "def char_ngrams(s: str, n: int = 3):\n",
    "    s = s.replace(' ', ' ')\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "def top_char_ngrams(df: pd.DataFrame, col: str, n=3, top_k=30):\n",
    "    c = Counter()\n",
    "    for s in df[col].tolist():\n",
    "        c.update(char_ngrams(s, n))\n",
    "    print(f\"Top {top_k} char {n}-grams in {col}:\")\n",
    "    for gram, cnt in c.most_common(top_k):\n",
    "        print(f\"{gram}\\t{cnt}\")\n",
    "\n",
    "top_char_ngrams(train, SRC_COL, n=3, top_k=30)\n",
    "top_char_ngrams(train, TGT_COL, n=3, top_k=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
